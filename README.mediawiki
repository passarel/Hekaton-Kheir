The Hecatonchires, or Hekatonkheires, were three gargantuan figures of Greek mythology. Their name derives from the Greek hekaton ("hundred") and kheir ("hand"), and means "Hundred-Handed", "each of them having a hundred hands and fifty heads" (Bibliotheca). They were giants of incredible strength and ferocity, even superior to that of the Titans and the Cyclopes. They were children of Gaia and Uranus, simply the issue of Earth and Sky. Soon after they were born, their father, Uranus, threw them into the depths of Tartarus because he saw them as hideous monsters. (Source: wikipedia.org).

The Hekatonkheir project consists, basically, of a set of different resources to visualize and configure architectures of artificial neural networks (ANN) and of a (not so much) user-friendly interface to perform training, validation (according the method of cross validation) and application of the network, catering from different steps from handling the sets of input data until the visualization of the performance of the process. In terms of applications, it presents a series of interfaces to handle the integration with a logic programming language as well as dealing with representation and learning of temporal structures such as transition maps.

This project started at September, 2005, as a work for a post-graduate discipline. The main objective was to generate a visual interface to ANNs, allowing more clarity to visualize and edit specific configurations of them. Since then, different ideas has been incorporated to the project, with the main goal of integrating with neural-symbolic integration and applications in software engineering.

The name Hekatonkheir (maybe Frankenstein would be more adequated) is due the great amount of diferent modules and funcionalities integrated. When the first integrated tests were run, - and several bugs appeared, making me notice what a hideous monster that I had created - the image of Uranus when the Hekatonkheires were born came into my mind. Once, I told to a friend that finding a bug on the project was like find a flea behind the ear of a Hekatonkheir without knowing which head (the term "flea behind the ear" is a slang in Brazil, used to describe when somebody is worried or suspecting of something wrong).

This is an initial documentation for helping new users to start using the available functionalities, organized in the form of a "how-to". The following processes were defined until now:

* Creating and configuring a new neural network, with or without BG knowledge
* Training and validating a neural network
* Visualizing charts
* Learning state diagrams    

== How to ==

In this document, we expect to explain, in general, how to use some of the resources provided by the tool in order to generate neural network, train them and analyse the results of the application of the network in test cases.

=== Creating and configuring a neural network ===

In the main window of HekatonKheir, a visual illustration of current network is shown, as well as a menu to the configuration of general parameters of the network, and specific parameters of individual or groups of units. These files can be loaded and saved into a XML format. In the figure below, we illustrate this interface with a simple network being shown: this network has three layers of neurons. Notice that input and output units are shown in the network: these units are used by the interface only to illustrate which neurons are responsible for receiving the input values, and which are responsible to return the calculated output values for the network.

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/mainWindow.jpg|center|Main window of Hekaton Kheir]]

The menu in the right allows for the configuration of properties of the network, such as the style of training (i.e., the way data should be propagated through the recurrent links when training the network), the constants to define such styles, visual configuration and connection of units in a layered manner. Also, the menu have tabs that allow the individual configuration of units and connections from the network. Although inserting individual units is disabled in the main interface for this version, it is possible to insert them via menu.

However, for creating new networks, we make two different interfaces available. At first, we have the logic programming interface, which allow propositional and temporal logic programs to be translated into neural networks through CILP or SCTL algorithms. On the other hand, for the generation of traditional networks, there is a wizard available, where general parameters can be given so pre-defined styles of networks are created. This wizard is accessible by tools -> wizards menu, and illustrated below.

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/netWizard.jpg|center|100px|Wizard for the creation of new networks]]

=== Editing input files ===

In order to train the network with examples, an file with the data set needs to be provided. In the menu Training -> Input Files, we allow several operations to be performed into a simple text file, such as selecting inputs and outputs to be used, naming the columns, randomizing the rows, grouping the rows to be presented in sequence or dividing the data set into n subsets for n-fold cross validation purposes. This allow for most of the tasks associated with the preparation of the file to training purposes.

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/fileConfig.jpg|center|Configuration of input data file]]

When opening the screen, a dialog box is shown to select the original file containing the input data. This file can be any text file, containing the numeric data to be used, separated by spaces or tabs. The left-side of the screen will show a grid with the data, with the first row representing the labels of the columns (if existing), and the first column illustrating the subsets used for cross validation purposes. The right hand have three different tabs, containing respectively:

* Configuration of the columns labels, groups of rows to be kept together as a sequence, and configuration of each individual column, used to identify maximum and minimum values, label and its use as input, output or both during training. 
* Configuration of the subsets: allows to define the number of subsets used in cross-validation, as well as defining if these subsets should keep the proportional quantities of certain data patterns, according to given similarities. 
* Value editor (beta version): allows further edition of the values inside the grid, such as replacing values or replacing enumerable values by a propositional representation.

After performing the necessary configuration, closing the window (through the OK button) will open a new dialog to specify the file that keeps this information (.pid extension). This file type will then be used for training in different tools provided by our framework, as in the next section:

=== Training and validating a network ===

When selecting the menu option Training -> Execute, the screen shown below will allow for configuring and performing the training of a neural network. Basically, two kinds of training can be performed: training of the current network (the one being shown in the main interface) with the data on a selected .pid file, and the training of all the networks saved (as .xml) on a directory, using all the .pid files in the same directory (the used directories are specified in a text file).

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/trainConfig.jpg|center|Interface for configuration of training/cross validation]]

As many of the options are given by the file, few details remain to be set before performing the training. The first consists in defining the minimum and maximum values used to normalize the data of the input files, in the case where these values differ from the definition of the network (given by the input and output units) and the definition of the file. On the right side, remaining options are given, such as:

* Selective error calculation: only uses the outputs selected as 'main output' to calculate the output error in a group of rows (barely used)

* Selective training: only uses the outputs selected as 'main output' to the error correction (barely used)

* Keep training best epoch: when marked, the system will save the configuration of the epoch with smaller error in the training data. When not marked, it will save the configuration with smaller error on the test data.

* Randomize order in groups: randomize the sequence of the rows in the same group before using them to train the network

* Expand domain when checking: explores every possible sequence of rows in the same group when testing the network (may be to expensive computationally)

* Save networks: saves the final configuration of trained networks into a XML file, as given in the box below

* Number of epochs: Number of epochs that of each execution

* Error threshold: deprecated option, used to define correct classifications when printing an extra line in the report with this information

* Cross validation options: used to define how many subsets of the input data set should be used for training and how many should be used for test. It also allows for training only, without validation

After clicking the Execute button, a dialog box will open to select the file to save the report of that training. This report will then be saved under an extension .rrf (while an extra file with extension .rrf1 will be then created to keep more information about the evolution of the training). After this, the actual training will start, with a screen showing the progress of this process, and the evolution of the RMSE in a chart for each validation:

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/trainEvolution.jpg|center|Screen illustrating the evolution of a training execution]]

=== Analysing results ===

When using our tool, different results analysis were necessary to different applications. Our tool for analysing reports is an attempt to provide general functionalities to manipulate the data and generate charts. This interface can be open by selecting the menu Training -> Output Charts. Each report collected from training will have the desired and the obtained output values for each pattern (row of the input file), on the saved epoch. The left set of check boxes will allow the selection of which subsets are to be used to produce the report, where each subset represents one of the saved epoch of a specific validation. 

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/chartConfig.jpg|center|Interface to configure charts]]

The main area of the interface will allow processing and grouping the values of different columns. Each group will return a result of a function, such as sum, average, standard deviation, etc.. The application of such functions can take as consideration either the individual columns or the differences between consecutive columns. All these settings can be defined individually for each group. Also, data from different rows can be grouped, in a similar manner, according to the settings provided by the bottom panel.

After defining how the data from the report will be grouped for analysis, one can choose to save the result as a new file, or to show it as a chart. In the case of saving as a file, each group of columns for each subset will be saved as an individual column of the new report. On the other hand, when visualizing as a chart, each group of columns for each subset will be a different line series in the chart, as visualized in the screen shown below. The screen to visualize the charts allows to select which series should be visualized, as well as the color and the labels to be used. It also allows to save the charts as vetorial or bitmap images.

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/chartVisual.jpg|center|Interface to visualise charts]]

=== Learning temporal models of knowledge ===

The most important feature of Hekaton Kheir, which makes it different of other available neural network software, consists on its support to the integration of symbolic sources of knowledge. Besides allowing the translation of logic programs into neural networks (through the Logic... menu), Hekaton also makes available a tool for visualizing and learning state machines, accessible via Extra -> SE prototype menu option. In the figure below, we can see more details on how this interface is structured. 

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/stateDiagram.jpg|center|Interface to learn temporal models]]

The right hand side menu gives us a broad range of options to configure the learning environment and to perform adaptation tasks:

* Network tab: for loading a network from a XML file, and also allows setting some of the learning parameters

* Inputs tab: lists all the inputs in the domain, also allowing to unify sets of inputs to appear together in the state diagram

* States tab: in the same way as the inputs tab, but also allowing to set the X and Y position of each state represented in the diagram

* Transitions tab: list all the existing transitions on the chart, describing its attributes such as count and weight

* Data file tab: allows a .pid to be loaded, as well as configuring which data are to be considered in the learning process

* Supervision sequences: For the insertion of sequences that will supervise the learning process.

* Extra options: defines parameters to be used during execution, such as using the network output to define the next state, enabling learning and defining the correction of outputs when desired value is not given

* Defines the parameters for the execution, such as size of each epoch, number of epochs, rate for updating the diagram, and the percentage of inputs to be according to the supervision sequences (in the case of training based on sequences and random inputs)

Besides the visual editor, most of the configuration can be done through scripts (.mms files). These scripts will contain simple instructions that can be loaded through the menu File -> Load. Examples of these scripts can be found in the examples folders, also available on the repository. These instructions are in the format described below:

* add_input_variables: Inserts all the names in the given list as input variables
* add_state_variables: Inserts all the names in the given list as state variables
* set_scalar_inputs: Considers a set of input variables (given by their index) as being possible values of a scalar variable
* use_scalar_inputs: Indicates the use of scalar variables for the inputs
* set_state_x: Defines the X position of a given state in the diagram
* set_state_y: Defines the Y position of a given state in the diagram
* load_network: Loads a network from the file. A number of extra hidden neurons to be inserted can be given as extra parameter
* network_eta: Changes the learning rate from the network 
* use_file_none: Do not use an input file
* use_file_input: Uses only the input values given by the data file: the outputs are defined by the sequences
* use_file_all: Uses both input and output values from the data file
* load_sequences Defines the sequences that will be used for learning
* not_reinforce_current_output and reinforce_current_output: Options regarding the values to the outputs in the backpropagation when desired value is 0
* allow_learning and not_allow_learning: Options identifying if learning will be performed, or if the execution will only works for extracting the transitions
* reset_training: resets all the information regarding the learning process
* execute Receives, as parameters, the number of epochs, the size of epochs, the percentage of chosen inputs and the update rate of the diagram
* save_network Saves the network's configuration (incorporating learning effects)
* save_history Saves the history of last execution 
* save_logic_program and save_logic_program_xtra Saves the transitions extracted as a logic program
* save_RMSE_history Saves the calculated values of RMSE in a text file
* save_diagram_as_WMF Saves the current diagram as a .WMF (windows metafile) image
* save_RMSE_as_WMF Saves the RMSE chart as a .WMF image

== Example of framework's usage ==

=== Description of the example ===

 To this case, we will consider an example based on the configuration of a Pump System, in which signals from the environment are used to set information about methane level (if it is high or low), about water pressure (if high or low) and about the actual pump (if it is off or on). The input variables are:

* CrMeth
* HiWater
* PumpOn

There are many ways of building a network to represent this knowledge. We start by showing how to learn this knowledge from observable examples. In order to allow this learning to be performed in our system, we need to process an input file so it can be in the Hekaton .pid (processed input data) format. This file can then be used to train a network with the appropriate number of inputs and outputs. 

=== Example 1: running cross validation ===

In the folder examples/pumpNoisyCross, we have a set of files used for Cross Validation purposes using our interface. This process can be repeated as follows:

* Load the network FF_9_6_3.xml on the main screen
* Open any .txt file with the input data (Training -> Input File), and select the inputs and the outputs for using in training. This will generate a file with an extension .pid
* Open the Training -> Execute screen, select the pid file and the other options for generating the network: The result of the process will be saved in .rrf files, that can be analysed through the use of our charts visualization tool in Training -> Output Charts. The available .rrf files on the folder are results for the learning process for each individual .pid file.

=== Example 2: learning from examples using 'SE prototype' ===

In the folder examples/pumpNoisyBB, we have several results from training the network with examples, using the Extra -> SE prototype option on the menu. The file script.mms contains the full description of the experiment (it can be loaded by accessing the menu option File -> Load...). Basically, what is done in this example is learning an transition diagram from a network without background knowledge. This is done by the following steps:

* Load the desired network (.xml file). Loading the network will automatically configure the number of inputs and states, which can also be changed by the interface.
* Set the training data (.pid file) to be used. In this case, the option "Use full data" should also be selected, since only the examples are being used for training purposes. 
* Choose the execution options and run the learning process.

The results of the experiments described by the script are available in the same folder. The different input files refer to different noise levels existing on the data - these experiments were performed to evaluate the capacity of the network to learn a model in noisy environments.

=== Example 3: learning from sequences ===

In the examples/learnSequences... folders, we find examples of how to use our interface to learn from the description of the desired states after sequences of inputs. In our case, we take a counter example obtained in the model checking process (as explained below), and convert it into the sequence {} -> CritMeth -> HiWat -> PumpOn -> Â¬ PumpOn, which means that for any state, a sequence of signCritMeth, signHiWat, and turnPumpOn inputs should lead to a state where PumpOn is false. This is represented in our system as shown of figure below. In the script file, this is represented by the lines "load_sequences 1", identifying that there is one sequence to be loaded, a line "3" identifying that the sequence has 3 inputs, a line with the value of initial state "0 0 0" identifying that any value is accepted for the original state, and then a line for each input (in sequence) and the desired final state ("0 0 -1").

Each of the folders illustrates a different scenario for the original network. All the scenarios consist on a network with knowledge about the original model, but this knowledge was inserted into the network in a different manner:

* Networks in learnSequence1 experiment considered a neural network with configuration 9-6-3 (6 hidden neurons) which knowledge was build through training via examples. In order to evaluate the effect of having more neurons on the network, extra hidden neurons were inserted for each specific experiment

* Networks in learnSequence2 experiment are similar to the previous one, but instead of adding extra hidden neurons after learning the original model, these neurons were inserted before this first learning step, therefore they were already subject to a training process

* Networks in learnSequence3 experiment were generated by translating the symbolic description (.plg file) into a new network. As in the first case, a different number of extra hidden neurons was inserted for each experiment.

The obtained results for any case are also illustrated on the folder. The .wmf files illustrate the images of the diagrams and the error evolution charts, and the .plg files illustrate the extracted logic programs (simplification is not fully implemented yet)

=== Integration with model checking ===

In this section, we show how to integrate the learning capabilities of Hekaton Kheir neural networks with [http://nusmv.fbk.eu/ NuSMV model checker]. Most of these operations still need strong intervention of the developer. In the modelChecking folder, we have an original description in NuSMV for our model, which is shown in the file description1.smv. This file can be translated into a SCTL description (as shown in description1.plg), which can then be used to produce a new network (as in the case of learnSequence3 experiments). Verifying the given property G !((PumpOn & CritMeth) & HiWat) in NuSMV will then return the output shown in file execution1.txt. 

This result shown in file execution1.txt can then be analysed to generate the learning sequence shown in the last section. The learn process can then be deployed to adapt the model to that counter example. In the description2.smv and description2.plg files we will have these improved models. Again, a new verification can be performed, generating as result the content of the file exxecution2.txt. The sequence of these operations will finally get to the state where the property is verified, i.e, no counter example will be found for our verification process.
  
== Compiling the source ==

The Hekaton Kheir project is a software for academic and personal use, with open source (GPL license) and developed in Delphi Personal 7. In order to run the project in Delphi, however, some other libraries and components should be installed

* ImpStringGrid: developed by... available in...
* JEDI (JCL and JVCL), version...
* OpenXML...
* PngImage...
* SimpleGraph...
* XDOM...
