The Hecatonchires, or Hekatonkheires, were three gargantuan figures of Greek mythology. Their name derives from the Greek hekaton ("hundred") and kheir ("hand"), and means "Hundred-Handed", "each of them having a hundred hands and fifty heads" (Bibliotheca). They were giants of incredible strength and ferocity, even superior to that of the Titans and the Cyclopes. They were children of Gaia and Uranus, simply the issue of Earth and Sky. Soon after they were born, their father, Uranus, threw them into the depths of Tartarus because he saw them as hideous monsters. (Source: wikipedia.org).

The Hekatonkheir project consists, basically, of a set of different resources to visualize and configure architectures of artificial neural networks (ANN) and of a (not so much) user-friendly interface to perform training, validation (according the method of cross validation) and application of the network, catering from different steps from handling the sets of input data until the visualization of the performance of the process. In terms of applications, it presents a series of interfaces to handle the integration with a logic programming language as well as dealing with representation and learning of temporal structures such as transition maps.

This project started at September, 2005, as a work for a post-graduate discipline. The main objective was to generate a visual interface to ANNs, allowing more clarity to visualize and edit specific configurations of them. Since then, different ideas has been incorporated to the project, with the main goal of integrating with neural-symbolic integration and applications in software engineering.

The name Hekatonkheir (maybe Frankenstein would be more adequated) is due the great amount of diferent modules and funcionalities integrated. When the first integrated tests were run, - and several bugs appeared, making me notice what a hideous monster that I had created - the image of Uranus when the Hekatonkheires were born came into my mind. Once, I told to a friend that finding a bug on the project was like find a flea behind the ear of a Hekatonkheir without knowing which head (the term "flea behind the ear" is a slang in Brazil, used to describe when somebody is worried or suspecting of something wrong).

This is an initial documentation for helping new users to start using the available functionalities, organized in the form of a "how-to". The following processes were defined until now:

* Creating and configuring a new neural network, with or without BG knowledge
* Training and validating a neural network
* Visualizing charts
* Learning state diagrams    

== How to ==

In this document, we expect to explain, in general, how to use some of the resources provided by the tool in order to generate neural network, train them and analyse the results of the application of the network in test cases.

=== Creating and configuring a neural network ===

In the main window of HekatonKheir, a visual illustration of current network is shown, as well as a menu to the configuration of general parameters of the network, and specific parameters of individual or groups of units. These files can be loaded and saved into a XML format. In the figure below, we illustrate this interface with a simple network being shown: this network has three layers of neurons. Notice that input and output units are shown in the network: these units are used by the interface only to illustrate which neurons are responsible for receiving the input values, and which are responsible to return the calculated output values for the network.

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/mainWindow.jpg|center|Main window of Hekaton Kheir]]

The menu in the right allows for the configuration of properties of the network, such as the style of training (i.e., the way data should be propagated through the recurrent links when training the network), the constants to define such styles, visual configuration and connection of units in a layered manner. Also, the menu have tabs that allow the individual configuration of units and connections from the network. Although inserting individual units is disabled in the main interface for this version, it is possible to insert them via menu.

However, for creating new networks, we make two different interfaces available. At first, we have the logic programming interface, which allow propositional and temporal logic programs to be translated into neural networks through CILP or SCTL algorithms. On the other hand, for the generation of traditional networks, there is a wizard available, where general parameters can be given so pre-defined styles of networks are created. This wizard is accessible by tools -> wizards menu, and illustrated below.

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/netWizard.jpg|center|100px|Wizard for the creation of new networks]]

=== Editing input files ===

In order to train the network with examples, an file with the data set needs to be provided. In the menu Training -> Input Files, we allow several operations to be performed into a simple text file, such as selecting inputs and outputs to be used, naming the columns, randomizing the rows, grouping the rows to be presented in sequence or dividing the data set into n subsets for n-fold cross validation purposes. This allow for most of the tasks associated with the preparation of the file to training purposes.

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/fileConfig.jpg|center|Configuration of input data file]]

When opening the screen, a dialog box is shown to select the original file containing the input data. This file can be any text file, containing the numeric data to be used, separated by spaces or tabs. The left-side of the screen will show a grid with the data, with the first row representing the labels of the columns (if existing), and the first column illustrating the subsets used for cross validation purposes. The right hand have three different tabs, containing respectively:

* Configuration of the columns labels, groups of rows to be kept together as a sequence, and configuration of each individual column, used to identify maximum and minimum values, label and its use as input, output or both during training. 
* Configuration of the subsets: allows to define the number of subsets used in cross-validation, as well as defining if these subsets should keep the proportional quantities of certain data patterns, according to given similarities. 
* Value editor (beta version): allows further edition of the values inside the grid, such as replacing values or replacing enumerable values by a propositional representation.

After performing the necessary configuration, closing the window (through the OK button) will open a new dialog to specify the file that keeps this information (.pid extension). This file type will then be used for training in different tools provided by our framework, as in the next section:

=== Training and validating a network ===

When selecting the menu option Training -> Execute, the screen shown below will allow for configuring and performing the training of a neural network. Basically, two kinds of training can be performed: training of the current network (the one being shown in the main interface) with the data on a selected .pid file, and the training of all the networks saved (as .xml) on a directory, using all the .pid files in the same directory (the used directories are specified in a text file).

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/trainConfig.jpg|center|Interface for configuration of training/cross validation]]

As many of the options are given by the file, few details remain to be set before performing the training. The first consists in defining the minimum and maximum values used to normalize the data of the input files, in the case where these values differ from the definition of the network (given by the input and output units) and the definition of the file. On the right side, remaining options are given, such as:

* Selective error calculation: only uses the outputs selected as 'main output' to calculate the output error in a group of rows (barely used)

* Selective training: only uses the outputs selected as 'main output' to the error correction (barely used)

* Keep training best epoch: when marked, the system will save the configuration of the epoch with smaller error in the training data. When not marked, it will save the configuration with smaller error on the test data.

* Randomize order in groups: randomize the sequence of the rows in the same group before using them to train the network

* Expand domain when checking: explores every possible sequence of rows in the same group when testing the network (may be to expensive computationally)

* Save networks: saves the final configuration of trained networks into a XML file, as given in the box below

* Number of epochs: Number of epochs that of each execution

* Error threshold: deprecated option, used to define correct classifications when printing an extra line in the report with this information

* Cross validation options: used to define how many subsets of the input data set should be used for training and how many should be used for test. It also allows for training only, without validation

After clicking the Execute button, a dialog box will open to select the file to save the report of that training. This report will then be saved under an extension .rrf (while an extra file with extension .rrf1 will be then created to keep more information about the evolution of the training). After this, the actual training will start, with a screen showing the progress of this process, and the evolution of the RMSE in a chart for each validation:

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/trainEvolution.jpg|center|Screen illustrating the evolution of a training execution]]

=== Analysing results ===

When using our tool, different results analysis were necessary to different applications. Our tool for analysing reports is an attempt to provide general functionalities to manipulate the data and generate charts. This interface can be open by selecting the menu Training -> Output Charts. Each report collected from training will have the desired and the obtained output values for each pattern (row of the input file), on the saved epoch. The left set of check boxes will allow the selection of which subsets are to be used to produce the report, where each subset represents one of the saved epoch of a specific validation. 

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/chartConfig.jpg|center|Interface to configure charts]]

The main area of the interface will allow processing and grouping the values of different columns. Each group will return a result of a function, such as sum, average, standard deviation, etc.. The application of such functions can take as consideration either the individual columns or the differences between consecutive columns. All these settings can be defined individually for each group. Also, data from different rows can be grouped, in a similar manner, according to the settings provided by the bottom panel.

After defining how the data from the report will be grouped for analysis, one can choose to save the result as a new file, or to show it as a chart. In the case of saving as a file, each group of columns for each subset will be saved as an individual column of the new report. On the other hand, when visualizing as a chart, each group of columns for each subset will be a different line series in the chart, as visualized in the screen shown below. The screen to visualize the charts allows to select which series should be visualized, as well as the color and the labels to be used. It also allows to save the charts as vetorial or bitmap images.

[[image:http://github.com/passarel/Hekaton-Kheir/raw/master/doc/images/chartVisual.jpg|center|Interface to visualise charts]]

== Temporal Models of Knowledge ==

The most important feature of Hekaton Kheir consists on its support to the integration of symbolic sources of knowledge. In this section, we illustrate this features through the semi-automated process of integrating the learning capabilities of Hekaton Kheir neural networks with [http://nusmv.fbk.eu/ NuSMV model checker]. To this case, we will consider an example based on the configuration of a Pump System, in which signals from the environment are used to set information about methane level (if it is high or low), about water pressure (if high or low) and about the actual pump (if it is off or on). The input variables are:

* CrMeth
* HiWater
* PumpOn

There are many ways of building a network to represent this knowledge. We start by showing how to learn this knowledge from observable examples. In order to allow this learning to be performed in our system, we need to process an input file so it can be in the Hekaton .pid (processed input data) format. This file can then be used to train a network with the appropriate number of inputs and outputs. The file ... illustrates the original data used, while the file ... is the processed input data when considering....

Besides using the training interface as shown above, the state diagram interface can also be used in this case. In the figure below, we can see more details on how this interface is structured. The right hand side menu gives us a broad range of options to configure the learning environment and to perform adaptation tasks:

* Network tab: for loading a network from a XML file, and also allows setting some of the learning parameters

* Inputs tab: lists all the inputs in the domain, also allowing to unify sets of inputs to appear together in the state diagram

* States tab: in the same way as the inputs tab, but also allowing to set the X and Y position of each state represented in the diagram

* Transitions tab: list all the existing transitions on the chart, describing its attributes such as count and weight

* Data file tab: allows a .pid to be loaded, as well as configuring which data are to be considered in the learning process

* Supervision sequences: For the insertion of sequences that will supervise the learning process.

* Extra options: defines parameters to be used during execution, such as using the network output to define the next state, enabling learning and defining the correction of outputs when desired value is not given

* Defines the parameters for the execution, such as size of each epoch, number of epochs, rate for updating the diagram, and the percentage of inputs to be according to the supervision sequences (in the case of training based on sequences and random inputs)

As a first example of the system, lets consider the learning of the 

  
== Compiling the source ==

The Hekaton Kheir project is a software for academic and personal use, with open source (GPL license) and developed in Delphi Personal 7. In order to run the project in Delphi, however, some other libraries and components should be installed

* ImpStringGrid: developed by... available in...
* JEDI (JCL and JVCL), version...
* OpenXML...
* PngImage...
* SimpleGraph...
* XDOM...
